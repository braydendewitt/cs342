{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Tidzmt65Nm9O",
        "outputId": "be580660-1504-4c21-ddf4-9e9a59f397ac",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Populating the interactive namespace from numpy and matplotlib\n",
            "Collecting PySuperTuxKart\n",
            "  Downloading PySuperTuxKart-1.1.2-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (4.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.4/4.4 MB\u001b[0m \u001b[31m15.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting PySuperTuxKartData (from PySuperTuxKart)\n",
            "  Downloading PySuperTuxKartData-1.0.0.tar.gz (2.6 kB)\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from PySuperTuxKartData->PySuperTuxKart) (2.31.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->PySuperTuxKartData->PySuperTuxKart) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->PySuperTuxKartData->PySuperTuxKart) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->PySuperTuxKartData->PySuperTuxKart) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->PySuperTuxKartData->PySuperTuxKart) (2024.2.2)\n",
            "Building wheels for collected packages: PySuperTuxKartData\n",
            "\u001b[31mERROR: Operation cancelled by user\u001b[0m\u001b[31m\n",
            "\u001b[0m  Building wheel for PySuperTuxKartData (pyproject.toml) ... \u001b[?25l\u001b[?25hCollecting ray\n",
            "  Downloading ray-2.11.0-cp310-cp310-manylinux2014_x86_64.whl (65.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.3/65.3 MB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: click>=7.0 in /usr/local/lib/python3.10/dist-packages (from ray) (8.1.7)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from ray) (3.13.4)\n",
            "Requirement already satisfied: jsonschema in /usr/local/lib/python3.10/dist-packages (from ray) (4.19.2)\n",
            "Requirement already satisfied: msgpack<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from ray) (1.0.8)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from ray) (24.0)\n",
            "Requirement already satisfied: protobuf!=3.19.5,>=3.15.3 in /usr/local/lib/python3.10/dist-packages (from ray) (3.20.3)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from ray) (6.0.1)\n",
            "Requirement already satisfied: aiosignal in /usr/local/lib/python3.10/dist-packages (from ray) (1.3.1)\n",
            "Requirement already satisfied: frozenlist in /usr/local/lib/python3.10/dist-packages (from ray) (1.4.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from ray) (2.31.0)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema->ray) (23.2.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema->ray) (2023.12.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema->ray) (0.34.0)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema->ray) (0.18.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->ray) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->ray) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->ray) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->ray) (2024.2.2)\n",
            "Installing collected packages: ray\n",
            "Successfully installed ray-2.11.0\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'pystk'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-c64ce188ad1a>\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mpystk\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mray\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cuda'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cpu'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pystk'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ],
      "source": [
        "%pylab inline\n",
        "%pip install -U PySuperTuxKart\n",
        "%pip install ray\n",
        "import torch\n",
        "import sys, os\n",
        "import pystk\n",
        "import ray\n",
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "print('device = ', device)\n",
        "ray.init(logging_level=50)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PIKX4OEMNm9Q"
      },
      "outputs": [],
      "source": [
        "@ray.remote\n",
        "class Rollout:\n",
        "    def __init__(self, screen_width, screen_height, hd=True, track='lighthouse', render=True, frame_skip=1):\n",
        "        # Init supertuxkart\n",
        "        if not render:\n",
        "            config = pystk.GraphicsConfig.none()\n",
        "        elif hd:\n",
        "            config = pystk.GraphicsConfig.hd()\n",
        "        else:\n",
        "            config = pystk.GraphicsConfig.ld()\n",
        "        config.screen_width = screen_width\n",
        "        config.screen_height = screen_height\n",
        "        pystk.init(config)\n",
        "\n",
        "        self.frame_skip = frame_skip\n",
        "        self.render = render\n",
        "        race_config = pystk.RaceConfig(track=track)\n",
        "        self.race = pystk.Race(race_config)\n",
        "        self.race.start()\n",
        "\n",
        "    def __call__(self, agent, n_steps=200):\n",
        "        torch.set_num_threads(1)\n",
        "        self.race.restart()\n",
        "        self.race.step()\n",
        "        data = []\n",
        "        track_info = pystk.Track()\n",
        "        track_info.update()\n",
        "\n",
        "        for i in range(n_steps // self.frame_skip):\n",
        "            world_info = pystk.WorldState()\n",
        "            world_info.update()\n",
        "\n",
        "            # Gather world information\n",
        "            kart_info = world_info.players[0].kart\n",
        "\n",
        "            agent_data = {'track_info': track_info, 'kart_info': kart_info}\n",
        "            if self.render:\n",
        "                agent_data['image'] = np.array(self.race.render_data[0].image)\n",
        "\n",
        "            # Act\n",
        "            action = agent(**agent_data)\n",
        "            agent_data['action'] = action\n",
        "\n",
        "            # Take a step in the simulation\n",
        "            for it in range(self.frame_skip):\n",
        "                self.race.step(action)\n",
        "\n",
        "            # Save all the relevant data\n",
        "            data.append(agent_data)\n",
        "        return data\n",
        "\n",
        "def show_video(frames, fps=30):\n",
        "    import imageio\n",
        "    from IPython.display import Video, display\n",
        "\n",
        "    imageio.mimwrite('/tmp/test.mp4', frames, fps=fps, bitrate=1000000)\n",
        "    display(Video('/tmp/test.mp4', width=800, height=600, embed=True))\n",
        "\n",
        "viz_rollout = Rollout.remote(400, 300)\n",
        "def show_agent(agent, n_steps=600):\n",
        "    data = ray.get(viz_rollout.__call__.remote(agent, n_steps=n_steps))\n",
        "    show_video([d['image'] for d in data])\n",
        "\n",
        "rollouts = [Rollout.remote(50, 50, hd=False, render=False, frame_skip=5) for i in range(10)]\n",
        "def rollout_many(many_agents, **kwargs):\n",
        "    ray_data = []\n",
        "    for i, agent in enumerate(many_agents):\n",
        "         ray_data.append( rollouts[i % len(rollouts)].__call__.remote(agent, **kwargs) )\n",
        "    return ray.get(ray_data)\n",
        "\n",
        "def dummy_agent(**kwargs):\n",
        "    action = pystk.Action()\n",
        "    action.acceleration = 1\n",
        "    return action\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PtGTXlLXNm9Q"
      },
      "outputs": [],
      "source": [
        "def three_points_on_track(distance, track):\n",
        "    distance = np.clip(distance, track.path_distance[0,0], track.path_distance[-1,1]).astype(np.float32)\n",
        "    valid_node = (track.path_distance[..., 0] <= distance) & (distance <= track.path_distance[..., 1])\n",
        "    valid_node_idx, = np.where(valid_node)\n",
        "    node_idx = valid_node_idx[0] # np.random.choice(valid_node_idx)\n",
        "    d = track.path_distance[node_idx].astype(np.float32)\n",
        "    x = track.path_nodes[node_idx][:,[0,2]].astype(np.float32) # Ignore the y coordinate\n",
        "    w, = track.path_width[node_idx].astype(np.float32)\n",
        "\n",
        "    t = (distance - d[0]) / (d[1] - d[0])\n",
        "    mid = x[1] * t + x[0] * (1 - t)\n",
        "    x10 = (x[1] - x[0]) / np.linalg.norm(x[1]-x[0])\n",
        "    x10_ortho = np.array([-x10[1],x10[0]], dtype=float32)\n",
        "    return mid - w / 2 * x10_ortho, mid, mid + w / 2 * x10_ortho\n",
        "\n",
        "\n",
        "def state_features(track_info, kart_info, absolute=False, **kwargs):\n",
        "    f = np.concatenate([three_points_on_track(kart_info.distance_down_track + d, track_info) for d in [0,5,10,15,20]])\n",
        "    if absolute:\n",
        "        return f\n",
        "    p = np.array(kart_info.location)[[0,2]].astype(np.float32)\n",
        "    t = np.array(kart_info.front)[[0,2]].astype(np.float32)\n",
        "    f = f - p[None]\n",
        "    d = (p-t) / np.linalg.norm(p-t)\n",
        "    d_o = np.array([-d[1], d[0]], dtype=float32)\n",
        "    return np.stack([f.dot(d), f.dot(d_o)], axis=1)\n",
        "\n",
        "# Let's load a fancy auto-pilot. You'll write one yourself in your homework.\n",
        "from _auto_pilot import auto_pilot\n",
        "data, = rollout_many([auto_pilot], n_steps=400)\n",
        "\n",
        "figure()\n",
        "f = state_features(**data[50])\n",
        "plot(f[:,1].flat, f[:,0].flat, '*')\n",
        "axis('equal')\n",
        "gca().invert_yaxis()\n",
        "\n",
        "figure()\n",
        "for d in data:\n",
        "    f = state_features(**d, absolute=True)\n",
        "    plot(f[:,1].flat, f[:,0].flat, '*')\n",
        "axis('equal')\n",
        "gca().invert_yaxis()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g2LGkhVNNm9R"
      },
      "outputs": [],
      "source": [
        "from torch.distributions import Bernoulli # therefore, actions are steer left or steer right\n",
        "\n",
        "def new_action_net():\n",
        "    return torch.nn.Linear(2*5*3, 1, bias=False) # simple linear network, produces one output (positive = steer right, negative = steer left)\n",
        "\n",
        "class Actor: # Actor\n",
        "    def __init__(self, action_net):\n",
        "        self.action_net = action_net.cpu().eval() # initialize action net\n",
        "\n",
        "    def __call__(self, track_info, kart_info, **kwargs): # track info and kart info\n",
        "        f = state_features(track_info, kart_info) # get features\n",
        "        output = self.action_net(torch.as_tensor(f).view(1,-1))[0] # call action net to get output, reshape to have batch size of 1\n",
        "\n",
        "        action = pystk.Action() # initialize action\n",
        "        action.acceleration = 1 # acceleration = 1\n",
        "        steer_dist = Bernoulli(logits=output[0]) # Bernoulli distribution over output of network\n",
        "        action.steer = steer_dist.sample()*2-1 # sample steering command from this bernoulli distribution\n",
        "        return action # return action\n",
        "\n",
        "class GreedyActor: # Greedy actor\n",
        "    def __init__(self, action_net):\n",
        "        self.action_net = action_net.cpu().eval()\n",
        "\n",
        "    def __call__(self, track_info, kart_info, **kwargs):\n",
        "        f = state_features(track_info, kart_info)\n",
        "        output = self.action_net(torch.as_tensor(f).view(1,-1))[0]\n",
        "\n",
        "        action = pystk.Action()\n",
        "        action.acceleration = 1\n",
        "        action.steer = output[0] # threshold\n",
        "        return action"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aSpNwUdqNm9R"
      },
      "outputs": [],
      "source": [
        "action_net = new_action_net()\n",
        "show_agent(Actor(action_net))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7a3M4IpeNm9R"
      },
      "outputs": [],
      "source": [
        "# Can optimize more by generating many action nets, measure performance of all of them\n",
        "many_action_nets = [new_action_net() for i in range(10)] #\n",
        "\n",
        "data = rollout_many([Actor(action_net) for action_net in many_action_nets], n_steps=600) # use the actors, collect the data\n",
        "\n",
        "good_initialization = many_action_nets[ np.argmax([d[-1]['kart_info'].overall_distance for d in data]) ] # look at overall distance kart traveled, pick the best (furthest distance traveled)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9L-ZEsYHNm9R"
      },
      "outputs": [],
      "source": [
        "show_agent(Actor(good_initialization)) # use the good agent from above"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P7yyXTiNNm9R"
      },
      "source": [
        "Recall what we're trying to do in RL: maximize the expected return of a policy $\\pi$ (or in turn minmize a los $L$)\n",
        "$$\n",
        "-L = E_{\\tau \\sim P_\\pi}[R(\\tau)],\n",
        "$$\n",
        "where $\\tau = \\{s_0, a_0, s_1, a_1, \\ldots\\}$ is a trajectory of states and actions.\n",
        "The return of a trajectory is then defined as the sum of individual rewards $R(\\tau) = \\sum_k r(s_k)$ (we won't discount in this assignment).\n",
        "\n",
        "Policy gradient computes the gradient of the loss $L$ using the log-derivative trick\n",
        "$$\n",
        "\\nabla_\\pi L = -E_{\\tau \\sim P_\\pi}[\\sum_k r(s_k) \\nabla_\\pi \\sum_i \\log \\pi(a_i | s_i)].\n",
        "$$\n",
        "Since the return $r(s_k)$ only depends on action $a_i$ in the past $i < k$ we can further simplify the above equation:\n",
        "$$\n",
        "\\nabla_\\pi L = -E_{\\tau \\sim P_\\pi}\\left[\\sum_i \\left(\\nabla_\\pi \\log \\pi(a_i | s_i)\\right)\\left(\\sum_{k=i}^{i+T} r(s_k) \\right)\\right].\n",
        "$$\n",
        "We will implement an estimator for this objective below. There are a few steps that we need to follow:\n",
        "\n",
        " * The expectation $E_{\\tau \\sim P_\\pi}$ are rollouts of our policy\n",
        " * The log probability $\\log \\pi(a_i | s_i)$ uses the `Categorical.log_prob`\n",
        " * Gradient computation uses the `.backward()` function\n",
        " * The gradient $\\nabla_\\pi L$ is then used in a standard optimizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lb2yGtk6Nm9S"
      },
      "outputs": [],
      "source": [
        "# Implement REINFORCE\n",
        "\n",
        "import copy\n",
        "\n",
        "n_epochs = 20\n",
        "n_trajectories = 10 # trajectories to collect every epoch\n",
        "n_iterations =50 # number of iterations to train for\n",
        "batch_size = 128\n",
        "T = 20 # fixed horizon return - look at how much better the kart is doing in 20 steps from now (determines if current action is good or not)\n",
        "\n",
        "action_net = copy.deepcopy(good_initialization) # copy good initialization\n",
        "\n",
        "best_action_net = copy.deepcopy(action_net) # copy best action network (for now it is our good initialization)\n",
        "\n",
        "optim = torch.optim.Adam(action_net.parameters(), lr=1e-3) # create optimizer\n",
        "\n",
        "# training loop with REINFORCE...\n",
        "# need to compute the gradient w.r.t. loss w.r.t. policy\n",
        "\n",
        "for epoch in range(n_epochs):\n",
        "    eps = 1e-2\n",
        "\n",
        "    # Roll out the policy, compute the Expectation\n",
        "    trajectories = rollout_many([Actor(action_net)]*n_trajectories, n_steps=600) # rollout trajectories with actor based on action_net, n_trajectories for 600 timesteps\n",
        "    print('epoch = %d   best_dist = '%epoch, np.max([t[-1]['kart_info'].overall_distance for t in trajectories]))\n",
        "\n",
        "    # Compute all the reqired quantities to update the policy\n",
        "    features = []\n",
        "    returns = []\n",
        "    actions = []\n",
        "    for trajectory in trajectories:\n",
        "        for i in range(len(trajectory)): # go to each time step for each trajectory\n",
        "            # Compute the returns - return in this case is proportional to overall distance traveled in t time steps vs. overall distance traveled now, cap at length of trajectory to not get out of bounds error\n",
        "            returns.append( trajectory[min(i+T, len(trajectory)-1)]['kart_info'].overall_distance -\n",
        "                            trajectory[i]['kart_info'].overall_distance )\n",
        "            # Compute the features - features of the current trajectory, put onto GPU\n",
        "            features.append( torch.as_tensor(state_features(**trajectory[i]), dtype=torch.float32).cuda().view(-1) )\n",
        "            # Store the action that we took - store actions that the agent took\n",
        "            actions.append( trajectory[i]['action'].steer > 0 ) # steering is positive, we chose action 1; else if negative, chose action 0\n",
        "\n",
        "    # Upload everything to the GPU\n",
        "    returns = torch.as_tensor(returns, dtype=torch.float32).cuda() # ensure all have the right data type\n",
        "    actions = torch.as_tensor(actions, dtype=torch.float32).cuda()\n",
        "    features = torch.stack(features).cuda()\n",
        "\n",
        "    returns = (returns - returns.mean()) / returns.std() # normalize the returns by subtracting mean and dividing by the standard deviation\n",
        "\n",
        "    action_net.train().cuda() # Put network in training mode on GPU\n",
        "    avg_expected_log_return = []\n",
        "    for it in range(n_iterations): # iterate a few iterations of training\n",
        "        batch_ids = torch.randint(0, len(returns), (batch_size,), device=device) # take random subset of data (of returns, actions, and features per batch)\n",
        "        batch_returns = returns[batch_ids]\n",
        "        batch_actions = actions[batch_ids]\n",
        "        batch_features = features[batch_ids]\n",
        "\n",
        "        output = action_net(batch_features) # feed features through network\n",
        "        pi = Bernoulli(logits=output[:,0]) # bernoulli distribution of the network output\n",
        "\n",
        "        expected_log_return = (pi.log_prob(batch_actions)*batch_returns).mean() # log probability of the policy * the returns we chose, and take mean of all of that\n",
        "        optim.zero_grad() # call optimizer to zero gradient\n",
        "        (-expected_log_return).backward() # take negative expected log return and call backward on it\n",
        "        optim.step() # step the optimizer\n",
        "        avg_expected_log_return.append(float(expected_log_return)) # save all expected returns to make sure we are actually learning something\n",
        "\n",
        "    # How well policy perforns\n",
        "    # call rollout_many and compre the greedy actor of the current best policy, the best action net, with the greedy actor of the current action net; do this for 600 steps\n",
        "    best_performance, current_performance = rollout_many([GreedyActor(best_action_net), GreedyActor(action_net)], n_steps=600)\n",
        "    if best_performance[-1]['kart_info'].overall_distance < current_performance[-1]['kart_info'].overall_distance: # look at performance of the last step\n",
        "        best_action_net = copy.deepcopy(action_net) # if it is better (current distance is larger than last, we update the policy)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OjF16gcSNm9S"
      },
      "outputs": [],
      "source": [
        "show_agent(GreedyActor(best_action_net))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zsW1ojXYNm9S"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}